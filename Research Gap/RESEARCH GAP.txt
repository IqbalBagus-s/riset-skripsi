🧠 Latar Belakang Penelitian

Perkembangan teknologi deep learning dalam bidang computer vision telah menghasilkan berbagai arsitektur jaringan saraf konvolusional (Convolutional Neural Network / CNN) yang mampu mencapai performa tinggi pada tugas klasifikasi citra. Salah satu arsitektur CNN yang banyak digunakan adalah ResNet-18, yang diperkenalkan oleh He et al. (2015) dengan konsep skip connection atau residual learning untuk mengatasi masalah vanishing gradient pada jaringan yang dalam.

Meskipun ResNet-18 memiliki kemampuan representasi fitur yang baik, model ini masih memiliki keterbatasan dalam hal kemampuan untuk secara adaptif memfokuskan perhatian pada fitur penting dalam citra. CNN tradisional memproses semua area citra dengan bobot yang relatif seragam, sehingga bagian citra yang kurang relevan tetap berkontribusi terhadap hasil klasifikasi. Untuk mengatasi hal tersebut, berbagai penelitian telah memperkenalkan mekanisme attention, salah satunya adalah Convolutional Block Attention Module (CBAM) yang diusulkan oleh Woo et al. (2018). CBAM bekerja dengan dua tahap perhatian: channel attention dan spatial attention, yang memungkinkan jaringan untuk lebih fokus pada area dan fitur yang signifikan.

Sejumlah studi menunjukkan bahwa penerapan CBAM dapat meningkatkan akurasi pada dataset besar seperti ImageNet atau CIFAR-100. Namun, belum banyak penelitian yang secara spesifik menganalisis performa ResNet-18 + CBAM pada dataset berskala menengah dan kompleks seperti Tiny ImageNet, yang terdiri dari 200 kelas dan memiliki ukuran citra yang lebih kecil (64×64 piksel). Karakteristik ini membuat Tiny ImageNet menjadi dataset menantang bagi model CNN karena keterbatasan resolusi dan keberagaman kelas.

Berdasarkan kondisi tersebut, penelitian ini berfokus pada analisis perbandingan antara model ResNet-18 dan ResNet-18 yang dimodifikasi dengan CBAM untuk mengukur sejauh mana modul attention dapat meningkatkan kemampuan model dalam mengenali objek pada dataset Tiny ImageNet.

🔍 Identifikasi Gap Penelitian

Berdasarkan hasil telaah literatur dan perkembangan riset terkini, dapat diidentifikasi beberapa kesenjangan (research gap) sebagai berikut:

Kesenjangan penerapan konteks dataset:
Sebagian besar penelitian CBAM berfokus pada dataset besar (misalnya ImageNet, COCO, atau CIFAR-100), sementara benchmark pada dataset skala menengah seperti Tiny ImageNet masih jarang dilakukan.

Kesenjangan komparatif antar model dasar dan modifikasi:
Belum ada kajian yang membandingkan secara sistematis performa ResNet-18 dan ResNet-18 + CBAM dalam aspek akurasi validasi, loss convergence, dan efisiensi waktu pelatihan pada dataset Tiny ImageNet.

Kesenjangan analisis interpretabilitas model:
Sebagian penelitian hanya menyoroti peningkatan akurasi, tetapi belum banyak yang menganalisis bagaimana peta perhatian (attention map) CBAM mempengaruhi fokus model terhadap area citra yang relevan.

Kesenjangan empiris pada integrasi modul attention ringan:
CBAM dikenal sebagai modul yang ringan dan mudah diintegrasikan, tetapi efeknya pada model berkapasitas kecil (seperti ResNet-18) dengan dataset menengah belum banyak dibuktikan secara empiris.

🎯 Signifikansi Penelitian

Penelitian ini memiliki signifikansi pada dua aspek utama:

Aspek akademik: memberikan kontribusi terhadap pengembangan kajian attention mechanism pada model CNN, khususnya dalam konteks dataset dengan skala menengah.

Aspek praktis: memberikan panduan bagi peneliti dan praktisi dalam memilih kombinasi arsitektur CNN dan modul attention yang efisien untuk tugas klasifikasi citra dengan keterbatasan sumber daya komputasi.